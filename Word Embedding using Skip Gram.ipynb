{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwvN9VoX5ecUbRJNdpH0lY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shamshad-Gilani/NLP-Projects/blob/main/Word%20Embedding%20using%20Skip%20Gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evn7hZKsf7Pt",
        "outputId": "dfe8ff0b-f58e-40d3-d087-c9b31a0c3d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk import bigrams\n",
        "from collections import Counter, defaultdict\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Download and extract the zip file\n",
        "url = 'https://www.kaggle.com/api/v1/datasets/download/unitednations/un-general-debates'\n",
        "response = requests.get(url)\n",
        "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "zip_file.extractall('archive')\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = 'archive/un-general-debates.csv'\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Lowercasing\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    # Stop Words Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Combine all text data into a single corpus\n",
        "corpus = ' '.join(data['text'].tolist())\n",
        "\n",
        "# Preprocess the corpus\n",
        "tokens = preprocess(corpus)\n",
        "print(\"Preprocessed tokens:\", tokens[:50])  # Print the first 50 tokens for reference\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CncJKU6QgQc8",
        "outputId": "9e7eaa2b-8881-48a4-98a4-be6d835a3469"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed tokens: ['\\ufeffit', 'inde', 'pleasur', 'member', 'deleg', 'extend', 'ambassador', 'garba', 'sincer', 'congratul', 'elect', 'presid', 'forty-fourth', 'session', 'gener', 'assembl', '.', 'elect', 'high', 'offic', 'well-deserv', 'tribut', 'person', 'qualiti', 'experi', '.', 'fulli', 'confid', 'abl', 'wise', 'leadership', 'assembl', 'consolid', 'gain', 'achiev', 'past', 'year', '.', 'deleg', 'associ', 'previou', 'speaker', 'express', 'appreci', 'dedic', 'effort', 'predecessor', ',', 'excel', 'mr.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3LRCz0LrO4X",
        "outputId": "a04a13a6-c34c-4a32-be3d-e855fd8c3b4e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Preprocess the data (Already done)\n",
        "\n",
        "# Train a Skip Gram model\n",
        "model = Word2Vec([tokens], vector_size=100, window=5, min_count=1, sg=1)\n"
      ],
      "metadata": {
        "id": "bQ2o8134rSKM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Function to get the vector of a document\n",
        "def document_vector(doc):\n",
        "    doc = preprocess(doc)\n",
        "    doc_vector = np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)\n",
        "    return doc_vector\n",
        "\n",
        "# Vectorize each document in the dataset\n",
        "data['vector'] = data['text'].apply(document_vector)\n",
        "\n",
        "# Calculate similarity between documents\n",
        "similarity_matrix = cosine_similarity(np.stack(data['vector'].values))\n"
      ],
      "metadata": {
        "id": "wcYLpf9frU5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the indices of the top two similar documents\n",
        "similar_docs = np.unravel_index(np.argsort(similarity_matrix, axis=None)[-3:-1], similarity_matrix.shape)\n",
        "\n",
        "top_similar_docs = [(data['text'][i], data['text'][j]) for i, j in zip(*similar_docs)]\n",
        "print(\"Top two similar documents:\", top_similar_docs)\n"
      ],
      "metadata": {
        "id": "o76gmYIYraMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce dimensionality to 2D\n",
        "pca = PCA(n_components=2)\n",
        "reduced_vectors = pca.fit_transform(np.stack(data['vector'].values))\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1])\n",
        "plt.title('Document Embeddings in 2D Space')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Kze6OS7ordSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}